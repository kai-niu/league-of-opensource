{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build toy model and save it to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kai/anaconda3/envs/dlnn/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "\n",
    "iris = load_iris()\n",
    "data, labels = iris.data, iris.target\n",
    "training_data, test_data, training_labels, test_labels = train_test_split(data, labels)\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")\n",
    "model.fit(training_data, training_labels)\n",
    "accuracy = model.score(test_data, test_labels)\n",
    "print(\"accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "# Upload the model\n",
    "\n",
    "pickle.dump(model, open(\"toy_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Predictor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictor.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "labels = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "\n",
    "\n",
    "class PythonPredictor:\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        if os.environ.get(\"AWS_ACCESS_KEY_ID\"):\n",
    "            s3 = boto3.client(\"s3\")  # client will use your credentials if available\n",
    "        else:\n",
    "            s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))  # anonymous client\n",
    "\n",
    "        s3.download_file(config[\"bucket\"], config[\"key\"], \"/tmp/model.pkl\")\n",
    "        \"\"\"\n",
    "        self.model = pickle.load(open(\"toy_model.pkl\", \"rb\"))\n",
    "\n",
    "    def predict(self, payload):\n",
    "        measurements = [\n",
    "            payload[\"sepal_length\"],\n",
    "            payload[\"sepal_width\"],\n",
    "            payload[\"petal_length\"],\n",
    "            payload[\"petal_width\"],\n",
    "        ]\n",
    "\n",
    "        label_id = self.model.predict([measurements])[0]\n",
    "        return labels[label_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cortex.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cortex.yaml\n",
    "- name: iris-model\n",
    "  kind: RealtimeAPI\n",
    "  predictor:\n",
    "    type: python\n",
    "    path: predictor.py\n",
    "    config:\n",
    "      bucket: foo-bucket\n",
    "      key: bar-folder/model-foobar.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Query Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample.json\n",
    "{\n",
    "  \"sepal_length\": 5.2,\n",
    "  \"sepal_width\": 3.6,\n",
    "  \"petal_length\": 1.5,\n",
    "  \"petal_width\": 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model\n",
    "deploy the model in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cortex deploy\n",
    "# !cortex get \n",
    "# !cortex get recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Deployed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><title>403: Forbidden</title><body>403: Forbidden</body></html>"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8888 -X POST -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
